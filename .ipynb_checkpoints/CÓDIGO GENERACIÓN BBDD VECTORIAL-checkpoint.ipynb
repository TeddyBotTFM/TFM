{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf3f8a75-e7bc-4c60-9239-b59e504cf884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62405f3d-2ada-4efa-a8a8-7bf5388ee6f5",
   "metadata": {},
   "source": [
    "# GENERACIÓN DE LA BASE DE DATOS VECTORIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb5a224e-4d17-4a96-bcf2-ac57a852b2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La colección 'OnlyContent_withStopwords' ya existe.\n",
      "La colección 'OnlyContent_withoutStopwords' ya existe.\n",
      "La colección 'Weighted_withStopwords' ya existe.\n",
      "La colección 'Weighted_withoutStopwords' ya existe.\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "import os\n",
    "\n",
    "# Connect to Qdrant cloud cluster\n",
    "url = os.getenv(\"QDRANT_URL\")  \n",
    "api_key = os.getenv(\"QDRANT_KEY\")\n",
    "qdrant = QdrantClient(url=url, api_key=api_key)\n",
    "\n",
    "#Se proporcionan las credenciales para poder acceder a Qdrant en la Memoria. \n",
    "\n",
    "# Define the collection names and vector size\n",
    "collection_names = [\n",
    "    \"OnlyContent_withStopwords\", \n",
    "    \"OnlyContent_withoutStopwords\", \n",
    "    \"Weighted_withStopwords\", \n",
    "    \"Weighted_withoutStopwords\"\n",
    "]\n",
    "vector_size = 768\n",
    "\n",
    "# Function to create a collection\n",
    "def create_collection_if_not_exists(collection_name):\n",
    "    try:\n",
    "        if not qdrant.collection_exists(collection_name=collection_name):\n",
    "            qdrant.create_collection(\n",
    "                collection_name=collection_name,\n",
    "                vectors_config={\n",
    "                    \"size\": vector_size,\n",
    "                    \"distance\": \"Euclid\",\n",
    "                }\n",
    "            )\n",
    "            print(f\"Colección '{collection_name}' creada correctamente.\")\n",
    "        else:\n",
    "            print(f\"La colección '{collection_name}' ya existe.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al configurar Qdrant para la colección '{collection_name}': {e}\")\n",
    "\n",
    "# Create all collections\n",
    "for name in collection_names:\n",
    "    create_collection_if_not_exists(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07a8a5ff-454b-4a40-a6ed-9802638cd8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados correctamente desde C:\\Users\\Adriana\\OneDrive\\Documents\\Master_DS\\TFM\\LIMPIEZA DE DATOS\\BBDD_limpia_sin_quitar_stopwords.csv.\n",
      "Datos cargados correctamente desde C:\\Users\\Adriana\\OneDrive\\Documents\\Master_DS\\TFM\\LIMPIEZA DE DATOS\\BBDD_limpia_quitados_stopwords.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file paths\n",
    "file_paths = [\n",
    "    \"DATOS\\BBDD_limpia_sin_quitar_stopwords.csv\",\n",
    "    \"DATOS\\BBDD_limpia_quitados_stopwords.csv\"\n",
    "]\n",
    "\n",
    "# Function to load a dataset\n",
    "def load_dataset(file_path):\n",
    "    try:\n",
    "        dataset = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "        print(f\"Datos cargados correctamente desde {file_path}.\")\n",
    "        return dataset\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error al cargar el archivo CSV en {file_path}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error desconocido al cargar el archivo CSV en {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load both datasets\n",
    "datasets = [load_dataset(path) for path in file_paths]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b23729d-c4f3-4283-8c92-c612aca9b17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_content_stopwords = datasets[0][['Contenido']] \n",
    "docs_content_NOstopwords = datasets[1][['Contenido']] \n",
    "docs_all_stopwords = datasets[0] \n",
    "docs_all_NOstopwords = datasets[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15ad4371-47f1-4352-b8f5-05387455c388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "from langchain.schema import Document\n",
    "\n",
    "loader_content_stopwords = DataFrameLoader(docs_content_stopwords, page_content_column=\"Contenido\")\n",
    "documents_content_stopwords = loader_content_stopwords.load()\n",
    "\n",
    "loader_content_NOstopwords = DataFrameLoader(docs_content_NOstopwords, page_content_column=\"Contenido\")\n",
    "documents_content_NOstopwords = loader_content_NOstopwords.load()\n",
    "\n",
    "def dataframe_to_documents_with_metadata(df, content_column):\n",
    "    documents = []\n",
    "    for _, row in df.iterrows():\n",
    "        content = row[content_column]\n",
    "        metadata = row.drop(labels=[content_column]).to_dict()\n",
    "        documents.append(Document(page_content=content, metadata=metadata))\n",
    "    return documents\n",
    "    \n",
    "documents_all_stopwords = dataframe_to_documents_with_metadata(docs_all_stopwords, content_column = 'Contenido')\n",
    "\n",
    "documents_all_NOstopwords = dataframe_to_documents_with_metadata(docs_all_NOstopwords, content_column = 'Contenido')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc49a063-503a-4be8-a1dd-da77ba494a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_texts = [doc.page_content for doc in documents_content_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8cde786-c1b9-45bf-a0da-88ad98d4eb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adriana\\OneDrive\\Documents\\Master_DS\\TFM\\TFM\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "C:\\Users\\Adriana\\OneDrive\\Documents\\Master_DS\\TFM\\TFM\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generados correctamente.\n",
      "Datos almacenados en Qdrant correctamente.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client.models import PointStruct\n",
    "\n",
    "# Load the SentenceTransformer model\n",
    "embeddings_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "# Connect to Qdrant cloud cluster\n",
    "url = os.getenv(\"QDRANT_URL\")\n",
    "api_key = os.getenv(\"QDRANT_KEY\")\n",
    "qdrant = QdrantClient(url=url, api_key=api_key)\n",
    "\n",
    "collection_name = \"OnlyContent_withStopwords\"\n",
    "# Generate embeddings\n",
    "try:\n",
    "    document_embeddings = embeddings_model.encode(document_texts)\n",
    "    print(\"Embeddings generados correctamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al generar embeddings: {e}\")\n",
    "\n",
    "try:\n",
    "# Create points for Qdrant\n",
    "    points = [\n",
    "        PointStruct(\n",
    "            id=i,\n",
    "            vector=embedding.tolist(),  \n",
    "            payload={\"text\": document_texts[i]}  \n",
    "        )\n",
    "        for i, embedding in enumerate(document_embeddings)\n",
    "]\n",
    "# Upload points to Qdrant\n",
    "    qdrant.upsert(\n",
    "        collection_name=collection_name,\n",
    "        points=points\n",
    "    )\n",
    "    print(\"Datos almacenados en Qdrant correctamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al almacenar los datos en Qdrant: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d955ec8-1ea0-4a4e-9145-5ba82370a197",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_texts = [doc.page_content for doc in documents_content_NOstopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb276c79-79cc-437a-8ef5-fc1830abd9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generados correctamente.\n",
      "Datos almacenados en Qdrant correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Load the SentenceTransformer model\n",
    "embeddings_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "# Connect to Qdrant cloud cluster\n",
    "url = os.getenv(\"QDRANT_URL\")\n",
    "api_key = os.getenv(\"QDRANT_KEY\")\n",
    "qdrant = QdrantClient(url=url, api_key=api_key)\n",
    "\n",
    "collection_name = \"OnlyContent_withoutStopwords\"\n",
    "# Generate embeddings\n",
    "try:\n",
    "    document_embeddings = embeddings_model.encode(document_texts)\n",
    "    print(\"Embeddings generados correctamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al generar embeddings: {e}\")\n",
    "\n",
    "try:\n",
    "# Create points for Qdrant\n",
    "    points = [\n",
    "        PointStruct(\n",
    "            id=i,\n",
    "            vector=embedding.tolist(),  \n",
    "            payload={\"text\": document_texts[i]}  \n",
    "        )\n",
    "        for i, embedding in enumerate(document_embeddings)\n",
    "]\n",
    "# Upload points to Qdrant\n",
    "    qdrant.upsert(\n",
    "        collection_name=collection_name,\n",
    "        points=points\n",
    "    )\n",
    "    print(\"Datos almacenados en Qdrant correctamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al almacenar los datos en Qdrant: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a538cb9-f275-425f-a17a-9d931b709701",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_texts = [doc for doc in documents_all_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bc4094e-a4ba-49a7-bb05-a14e31382bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generados correctamente.\n",
      "Datos almacenados en Qdrant correctamente.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client.models import PointStruct\n",
    "import numpy as np\n",
    "\n",
    "# Load the SentenceTransformer model\n",
    "embeddings_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "# Connect to Qdrant cloud cluster\n",
    "url = os.getenv(\"QDRANT_URL\")\n",
    "api_key = os.getenv(\"QDRANT_KEY\")\n",
    "qdrant = QdrantClient(url=url, api_key=api_key)\n",
    "\n",
    "collection_name = \"Weighted_withStopwords\"\n",
    "\n",
    "#Function to Combine Embeddings with Weights\n",
    "def weighted_embed_document(document, embedding_model, content_weight=0.7, title_weight=0.2, author_weight=0.1):\n",
    "    # Compute embeddings for each part\n",
    "    content_embedding = embedding_model.encode(document.page_content) * content_weight\n",
    "    title_embedding = embedding_model.encode(document.metadata.get('Title', '')) * title_weight\n",
    "    author_embedding = embedding_model.encode(document.metadata.get('Author', '')) * author_weight\n",
    "    \n",
    "    # Combine embeddings with the specified weights\n",
    "    combined_embedding = content_embedding + title_embedding + author_embedding\n",
    "    return combined_embedding\n",
    "    \n",
    "# Generate embeddings\n",
    "#Funtion to Embed All Documents\n",
    "def embed_documents(documents, embedding_model, content_weight=0.7, title_weight=0.2, author_weight=0.1):\n",
    "    embeddings = []\n",
    "    for doc in documents:\n",
    "        embedding = weighted_embed_document(doc, embedding_model, content_weight, title_weight, author_weight)\n",
    "        embeddings.append(embedding)\n",
    "    return embeddings\n",
    "\n",
    "try:\n",
    "    document_embeddings = embed_documents(document_texts, embeddings_model)\n",
    "    print(\"Embeddings generados correctamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al generar embeddings: {e}\")\n",
    "\n",
    "try:\n",
    "# Create points for Qdrant\n",
    "    points = [\n",
    "    PointStruct(\n",
    "        id=i,  # Unique identifier for each point\n",
    "        vector=embeddings.tolist(),  # Convert numpy.ndarray to list\n",
    "        payload={\n",
    "            \"content\": document.page_content,  # Main content\n",
    "            \"title\": document.metadata.get('Title', ''),  # Title metadata\n",
    "            \"author\": document.metadata.get('Author', '')  # Author metadata\n",
    "        }\n",
    "    )\n",
    "    for i, (embeddings, document) in enumerate(zip(document_embeddings, document_texts))\n",
    "]\n",
    "\n",
    "# Upload points to Qdrant\n",
    "    qdrant.upsert(\n",
    "        collection_name=collection_name,\n",
    "        points=points\n",
    "    )\n",
    "    print(\"Datos almacenados en Qdrant correctamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al almacenar los datos en Qdrant: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ef171e8-c891-4c17-8570-cba353ebc270",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_texts = [doc for doc in documents_all_NOstopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d400dc7f-4373-4387-90ca-9a22cd2ac3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generados correctamente.\n",
      "Datos almacenados en Qdrant correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Load the SentenceTransformer model\n",
    "embeddings_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "# Connect to Qdrant cloud cluster\n",
    "url = os.getenv(\"QDRANT_URL\")\n",
    "api_key = os.getenv(\"QDRANT_KEY\")\n",
    "qdrant = QdrantClient(url=url, api_key=api_key)\n",
    "\n",
    "collection_name = \"Weighted_withoutStopwords\"\n",
    "\n",
    "#Function to Combine Embeddings with Weights\n",
    "def weighted_embed_document(document, embedding_model, content_weight=0.7, title_weight=0.2, author_weight=0.1):\n",
    "    # Compute embeddings for each part\n",
    "    content_embedding = embedding_model.encode(document.page_content) * content_weight\n",
    "    title_embedding = embedding_model.encode(document.metadata.get('Title', '')) * title_weight\n",
    "    author_embedding = embedding_model.encode(document.metadata.get('Author', '')) * author_weight\n",
    "    \n",
    "    # Combine embeddings with the specified weights\n",
    "    combined_embedding = content_embedding + title_embedding + author_embedding\n",
    "    return combined_embedding\n",
    "    \n",
    "# Generate embeddings\n",
    "#Funtion to Embed All Documents\n",
    "def embed_documents(documents, embedding_model, content_weight=0.7, title_weight=0.2, author_weight=0.1):\n",
    "    embeddings = []\n",
    "    for doc in documents:\n",
    "        embedding = weighted_embed_document(doc, embedding_model, content_weight, title_weight, author_weight)\n",
    "        embeddings.append(embedding)\n",
    "    return embeddings\n",
    "\n",
    "try:\n",
    "    document_embeddings = embed_documents(document_texts, embeddings_model)\n",
    "    print(\"Embeddings generados correctamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al generar embeddings: {e}\")\n",
    "\n",
    "try:\n",
    "# Create points for Qdrant\n",
    "    points = [\n",
    "    PointStruct(\n",
    "        id=i,  # Unique identifier for each point\n",
    "        vector=embeddings.tolist(),  # Convert numpy.ndarray to list\n",
    "        payload={\n",
    "            \"content\": document.page_content,  # Main content\n",
    "            \"title\": document.metadata.get('Title', ''),  # Title metadata\n",
    "            \"author\": document.metadata.get('Author', '')  # Author metadata\n",
    "        }\n",
    "    )\n",
    "    for i, (embeddings, document) in enumerate(zip(document_embeddings, document_texts))\n",
    "]\n",
    "\n",
    "# Upload points to Qdrant\n",
    "    qdrant.upsert(\n",
    "        collection_name=collection_name,\n",
    "        points=points\n",
    "    )\n",
    "    print(\"Datos almacenados en Qdrant correctamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al almacenar los datos en Qdrant: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
